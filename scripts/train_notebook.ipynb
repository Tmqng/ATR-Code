{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfcfcfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "from absl import flags\n",
    "from absl import app\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils import tensorboard\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get the project root\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Add src/ to path\n",
    "sys.path.append(os.path.join(project_root, \"src\"))\n",
    "\n",
    "# modules in src\n",
    "from data.MSTAR.paper_AConvNet import preprocess\n",
    "from data.MSTAR.paper_AConvNet import loader\n",
    "from utils import common\n",
    "from models import AConvNet\n",
    "\n",
    "DATA_PATH = 'datasets/MSTAR/MSTAR_IMG_JSON'\n",
    "\n",
    "# DATA_PATH = 'datasets/MSTAR/mstar_data_paper_AConvNet/'\n",
    "\n",
    "model_str = 'AConvNet'\n",
    "\n",
    "\n",
    "# flags.DEFINE_string('experiments_path', os.path.join(common.project_root, 'experiments'), help='')\n",
    "# flags.DEFINE_string('config_name', f'{model_str}/config/AConvNet-SOC.json', help='')\n",
    "# FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "common.set_random_seed(12321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e73ef2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Start')\n",
    "# experiments_path = FLAGS.experiments_path\n",
    "# config_name = FLAGS.config_name\n",
    "\n",
    "# config = common.load_config(os.path.join(experiments_path, config_name))\n",
    "\n",
    "experiments_path = os.path.join(common.project_root, 'experiments')\n",
    "\n",
    "config = {\n",
    "  \"model_name\": \"AConvNet-SOC\",\n",
    "  \"dataset\": \"SOC\",\n",
    "  \"num_classes\": 10,\n",
    "  \"channels\": 1,\n",
    "  \"batch_size\": 100,\n",
    "  \"epochs\": 2,\n",
    "  \"momentum\": 0.9,\n",
    "  \"lr\": 1e-3,\n",
    "  \"lr_step\": [50],\n",
    "  \"lr_decay\": 0.1,\n",
    "  \"weight_decay\": 4e-3,\n",
    "  \"dropout_rate\": 0.5\n",
    "}\n",
    "\n",
    "dataset = config['dataset']\n",
    "classes = config['num_classes']\n",
    "channels = config['channels']\n",
    "epochs = config['epochs']\n",
    "batch_size = config['batch_size']\n",
    "\n",
    "lr = config['lr']\n",
    "lr_step = config['lr_step']\n",
    "lr_decay = config['lr_decay']\n",
    "\n",
    "weight_decay = config['weight_decay']\n",
    "dropout_rate = config['dropout_rate']\n",
    "\n",
    "model_name = config['model_name']\n",
    "\n",
    "# run(epochs, dataset, classes, channels, batch_size,\n",
    "#     lr, lr_step, lr_decay, weight_decay, dropout_rate,\n",
    "#     model_name, experiments_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8edcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, is_train, name, batch_size):\n",
    "    \"\"\"\n",
    "    Docstring for load_dataset\n",
    "    \n",
    "    :param path: Description\n",
    "    :param is_train: Description\n",
    "    :param name: Description\n",
    "    :param batch_size: Description\n",
    "\n",
    "    Load train, val or test dataset and apply transformations.\n",
    "    \"\"\"\n",
    "\n",
    "    val_transform = torchvision.transforms.Compose([preprocess.CenterCrop(94)])\n",
    "\n",
    "    train_transform = torchvision.transforms.Compose([preprocess.RandomCrop(94)])\n",
    "\n",
    "    _dataset = loader.Dataset(\n",
    "        path, name=name, is_train=is_train,\n",
    "        transform=None\n",
    "    )\n",
    "\n",
    "    if is_train:\n",
    "\n",
    "        # TODO Data_augmentation (in preprocess file)\n",
    "        print(f\"Augmenting training data with patches...\")\n",
    "        # Extract patches from training data\n",
    "        augmented_samples = preprocess.augment_dataset_with_patches(\n",
    "            _dataset,\n",
    "            # patch_size=patch_size,\n",
    "            # stride=stride,\n",
    "            # chip_size=chip_size,\n",
    "            desc=\"Train augmentation\"\n",
    "        )\n",
    "\n",
    "        print(f\"\\nRésultats augmentation :\")\n",
    "        print(f\"  Train : {len(_dataset)} images → {len(augmented_samples)} patches\")\n",
    "        print(f\"  Facteur : ~{len(augmented_samples) / len(_dataset):.0f}x (13x13 = 169 patches/image)\")\n",
    "\n",
    "        augmented_dataset = preprocess.AugmentedDataset(augmented_samples)\n",
    "\n",
    "        # augmented_dataset = _dataset\n",
    "\n",
    "        # Split into train (80%) and validation (20%)\n",
    "        train_size = int(0.8 * len(augmented_dataset))\n",
    "        val_size = len(augmented_dataset) - train_size\n",
    "\n",
    "        train_dataset, val_dataset = random_split(augmented_dataset, [train_size, val_size])\n",
    "\n",
    "        # CenterCrop for val and RandomCrop for train\n",
    "        train_dataset_transformed = preprocess.TransformWrapper(train_dataset, train_transform)\n",
    "        val_dataset_transformed = preprocess.TransformWrapper(val_dataset, val_transform)\n",
    "\n",
    "        train_data_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset_transformed, batch_size=batch_size, shuffle=is_train, num_workers=1\n",
    "        )\n",
    "\n",
    "        val_data_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset_transformed, batch_size=batch_size, shuffle=False, num_workers=1\n",
    "        )\n",
    "\n",
    "        return train_data_loader, val_data_loader\n",
    "\n",
    "\n",
    "    else:\n",
    "        test_dataset_transformed = preprocess.TransformWrapper(_dataset, val_transform)\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset_transformed, batch_size=batch_size, shuffle=is_train, num_workers=1\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation(m, ds):\n",
    "    num_data = 0\n",
    "    corrects = 0\n",
    "\n",
    "    # Test loop\n",
    "    m.net.eval()\n",
    "    _softmax = torch.nn.Softmax(dim=1)\n",
    "    for i, data in enumerate(tqdm(ds)):\n",
    "        if i == 0:\n",
    "            print(f\"Data structure: {type(data)}\")\n",
    "            print(f\"Data length: {len(data)}\")\n",
    "            images, labels, _ = data\n",
    "            print(f\"Images shape: {images.shape}\")\n",
    "            print(f\"Labels shape: {labels.shape}\")\n",
    "            print(f\"Unique labels in batch: {torch.unique(labels)}\")\n",
    "        images, labels, _ = data\n",
    "\n",
    "        images = images.to(m.device)\n",
    "        labels = labels.to(m.device)\n",
    "\n",
    "        predictions = m.inference(images)\n",
    "        predictions = predictions.to(m.device)\n",
    "        predictions = _softmax(predictions)\n",
    "\n",
    "        _, predictions = torch.max(predictions.data, 1)\n",
    "\n",
    "        # DEBUG: Check predictions\n",
    "        if i == 0:\n",
    "            print(f\"Predicted classes: {predictions[:10]}\")\n",
    "            print(f\"True labels: {labels[:10]}\")\n",
    "            print(f\"Matches: {(predictions == labels)[:10]}\")\n",
    "\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        num_data += labels.size(0)\n",
    "        corrects += (predictions == labels.to(m.device)).sum().item()\n",
    "\n",
    "    accuracy = 100 * corrects / num_data\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb55a890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load train data set:   0%|          | 0/2747 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load train data set: 100%|██████████| 2747/2747 [00:00<00:00, 4567.85it/s]\n",
      "load test data set: 100%|██████████| 2425/2425 [00:00<00:00, 4127.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# Décomposition de la fonction run\n",
    "\n",
    "# Load data\n",
    "train_set, val_set = load_dataset(DATA_PATH, True, dataset, batch_size)\n",
    "test_set = load_dataset(DATA_PATH, False, dataset, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PIE-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
