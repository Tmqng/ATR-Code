{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "MfcfSNaxMwKx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfcfSNaxMwKx",
        "outputId": "5334cdbf-ff01-4b6d-f0b9-0c75767984ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ATR-Code'...\n",
            "remote: Enumerating objects: 403, done.\u001b[K\n",
            "remote: Counting objects: 100% (403/403), done.\u001b[K\n",
            "remote: Compressing objects: 100% (229/229), done.\u001b[K\n",
            "remote: Total 403 (delta 165), reused 342 (delta 113), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (403/403), 6.94 MiB | 21.04 MiB/s, done.\n",
            "Resolving deltas: 100% (165/165), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Tmqng/ATR-Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "yPDjbN0mNJuC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPDjbN0mNJuC",
        "outputId": "7108b1cd-f38a-47d8-e72e-1ed0895d8957"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kaggle API version: \n",
            "Attempting to download: minhqunnguyen/mstar-images-et-json\n",
            "Dataset URL: https://www.kaggle.com/datasets/minhqunnguyen/mstar-images-et-json\n",
            "License(s): unknown\n",
            "Downloading mstar-images-et-json.zip to /content/ATR-Code/datasets/MSTAR\n",
            " 88% 240M/273M [00:00<00:00, 298MB/s]\n",
            "100% 273M/273M [00:00<00:00, 317MB/s]\n",
            "Extracting /content/ATR-Code/datasets/MSTAR/mstar-images-et-json.zip...\n",
            "✓ Done!\n",
            "✓ Download complete!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import json\n",
        "import subprocess\n",
        "import zipfile\n",
        "\n",
        "# Step 1: Set up Kaggle credentials\n",
        "kaggle_username = \"minhqunnguyen\"\n",
        "kaggle_api_key = \"KGAT_874d32201f32459c120dc5947083a3d8\"\n",
        "\n",
        "os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
        "\n",
        "kaggle_json = {\n",
        "    \"username\": kaggle_username,\n",
        "    \"key\": kaggle_api_key\n",
        "}\n",
        "\n",
        "json_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
        "with open(json_path, 'w') as f:\n",
        "    json.dump(kaggle_json, f)\n",
        "\n",
        "os.chmod(json_path, 0o600)\n",
        "\n",
        "# Step 2: Verify authentication works\n",
        "result = subprocess.run(['kaggle', 'api', '-v'], capture_output=True, text=True)\n",
        "print(\"Kaggle API version:\", result.stdout)\n",
        "\n",
        "# Step 3: Try downloading your private dataset\n",
        "dataset_identifier = \"minhqunnguyen/mstar-images-et-json\"\n",
        "output_dir = \"/content/ATR-Code/datasets/MSTAR/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Attempting to download: {dataset_identifier}\")\n",
        "!cd {output_dir} && kaggle datasets download -d {dataset_identifier}\n",
        "\n",
        "# Find the zip file\n",
        "zip_file = os.path.join(output_dir, \"mstar-images-et-json.zip\")\n",
        "\n",
        "if os.path.exists(zip_file):\n",
        "    print(f\"Extracting {zip_file}...\")\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(output_dir)\n",
        "    print(\"✓ Done!\")\n",
        "else:\n",
        "    print(f\"Zip file not found at {zip_file}\")\n",
        "    print(f\"Files in {output_dir}:\")\n",
        "    for f in os.listdir(output_dir):\n",
        "        print(f\"  - {f}\")\n",
        "\n",
        "print(\"✓ Download complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "WGdTkrAsySqQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGdTkrAsySqQ",
        "outputId": "ad2cc60e-420f-4219-9b23-e2442dd97a27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "os.chdir('/content/ATR-Code')\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cfcfcfb0",
      "metadata": {
        "id": "cfcfcfb0"
      },
      "outputs": [],
      "source": [
        "from absl import logging\n",
        "from absl import flags\n",
        "from absl import app\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.utils import tensorboard\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "import torchvision\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import json\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get the project root\n",
        "project_root = os.getcwd()\n",
        "\n",
        "# Add src/ to path\n",
        "sys.path.append(os.path.join(project_root, \"src\"))\n",
        "\n",
        "# modules in src\n",
        "from data.MSTAR.paper_AConvNet import preprocess\n",
        "from data.MSTAR.paper_AConvNet import loader\n",
        "from utils import common\n",
        "from models import AConvNet\n",
        "\n",
        "DATA_PATH = 'datasets/MSTAR/MSTAR_IMG_JSON'\n",
        "\n",
        "# DATA_PATH = 'datasets/MSTAR/mstar_data_paper_AConvNet/'\n",
        "\n",
        "model_str = 'AConvNet'\n",
        "\n",
        "\n",
        "# flags.DEFINE_string('experiments_path', os.path.join(common.project_root, 'experiments'), help='')\n",
        "# flags.DEFINE_string('config_name', f'{model_str}/config/AConvNet-SOC.json', help='')\n",
        "# FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "common.set_random_seed(12321)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e73ef2a4",
      "metadata": {
        "id": "e73ef2a4"
      },
      "outputs": [],
      "source": [
        "# experiments_path = FLAGS.experiments_path\n",
        "# config_name = FLAGS.config_name\n",
        "\n",
        "# config = common.load_config(os.path.join(experiments_path, config_name))\n",
        "\n",
        "experiments_path = os.path.join(common.project_root, 'experiments')\n",
        "\n",
        "config = {\n",
        "  \"model_name\": \"AConvNet-SOC\",\n",
        "  \"dataset\": \"SOC\",\n",
        "  \"num_classes\": 10,\n",
        "  \"channels\": 1,\n",
        "  \"batch_size\": 100,\n",
        "  \"epochs\": 5,\n",
        "  \"momentum\": 0.9,\n",
        "  \"lr\": 1e-3,\n",
        "  \"lr_step\": [50],\n",
        "  \"lr_decay\": 0.1,\n",
        "  \"weight_decay\": 4e-3,\n",
        "  \"dropout_rate\": 0.5\n",
        "}\n",
        "\n",
        "dataset = config['dataset']\n",
        "classes = config['num_classes']\n",
        "channels = config['channels']\n",
        "epochs = config['epochs']\n",
        "batch_size = config['batch_size']\n",
        "\n",
        "lr = config['lr']\n",
        "lr_step = config['lr_step']\n",
        "lr_decay = config['lr_decay']\n",
        "\n",
        "weight_decay = config['weight_decay']\n",
        "dropout_rate = config['dropout_rate']\n",
        "\n",
        "model_name = config['model_name']\n",
        "\n",
        "# run(epochs, dataset, classes, channels, batch_size,\n",
        "#     lr, lr_step, lr_decay, weight_decay, dropout_rate,\n",
        "#     model_name, experiments_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c8edcc8",
      "metadata": {
        "id": "7c8edcc8"
      },
      "outputs": [],
      "source": [
        "def load_dataset(path, is_train, name, batch_size, augment):\n",
        "    \"\"\"\n",
        "    Docstring for load_dataset\n",
        "    \n",
        "    :param path: Description\n",
        "    :param is_train: Description\n",
        "    :param name: Description\n",
        "    :param batch_size: Description\n",
        "\n",
        "    Load train, val or test dataset and apply transformations.\n",
        "    \"\"\"\n",
        "\n",
        "    if augment:\n",
        "        val_transform = torchvision.transforms.Compose([preprocess.CenterCrop(94)])\n",
        "        train_transform = torchvision.transforms.Compose([preprocess.RandomCrop(94)])\n",
        "    \n",
        "    else:\n",
        "        val_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), preprocess.CenterCrop(94)])\n",
        "        train_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), preprocess.RandomCrop(94)])\n",
        "\n",
        "    _dataset = loader.Dataset(\n",
        "        path, name=name, is_train=is_train,\n",
        "        transform=None\n",
        "    )\n",
        "\n",
        "    if is_train:\n",
        "\n",
        "        if augment:\n",
        "            # Data_augmentation (in preprocess file)\n",
        "            print(f\"Augmenting training data with patches...\")\n",
        "            # Extract patches from training data\n",
        "            augmented_samples = preprocess.augment_dataset_with_patches(\n",
        "                _dataset,\n",
        "                # patch_size=patch_size,\n",
        "                # stride=stride,\n",
        "                # chip_size=chip_size,\n",
        "                desc=\"Train augmentation\"\n",
        "            )\n",
        "\n",
        "            print(f\"\\nRésultats augmentation :\")\n",
        "            print(f\"  Train : {len(_dataset)} images → {len(augmented_samples)} patches\")\n",
        "            print(f\"  Facteur : ~{len(augmented_samples) / len(_dataset):.0f}x (13x13 = 169 patches/image)\")\n",
        "\n",
        "            augmented_dataset = preprocess.AugmentedDataset(augmented_samples)\n",
        "        else:\n",
        "            augmented_dataset = _dataset\n",
        "\n",
        "        # Split into train (80%) and validation (20%)\n",
        "        train_size = int(0.8 * len(augmented_dataset))\n",
        "        val_size = len(augmented_dataset) - train_size\n",
        "\n",
        "        train_dataset, val_dataset = random_split(augmented_dataset, [train_size, val_size])\n",
        "\n",
        "        for images, _, _ in train_dataset:\n",
        "            print(images.shape)\n",
        "            break\n",
        "\n",
        "        # CenterCrop for val and RandomCrop for train\n",
        "        train_dataset_transformed = preprocess.TransformWrapper(train_dataset, train_transform)\n",
        "        val_dataset_transformed = preprocess.TransformWrapper(val_dataset, val_transform)\n",
        "\n",
        "\n",
        "        for images, _, _ in train_dataset_transformed:\n",
        "            print(images.shape)\n",
        "            break\n",
        "        \n",
        "        train_data_loader = torch.utils.data.DataLoader(\n",
        "            train_dataset_transformed, batch_size=batch_size, shuffle=is_train, num_workers=1\n",
        "        )\n",
        "\n",
        "        val_data_loader = torch.utils.data.DataLoader(\n",
        "            val_dataset_transformed, batch_size=batch_size, shuffle=False, num_workers=1\n",
        "        )\n",
        "\n",
        "        # Check first batch\n",
        "        for images, labels, _ in train_data_loader:\n",
        "            print(f\"\\nFirst batch shapes:\")\n",
        "            print(f\"  Images: {images.shape}, dtype: {images.dtype}\")\n",
        "            print(f\"  Labels: {labels.shape}, dtype: {labels.dtype}\")\n",
        "            print(f\"  Labels values: {labels.tolist()[:10]}\")\n",
        "            print(f\"  Unique labels: {torch.unique(labels).tolist()}\")\n",
        "            break\n",
        "\n",
        "        return train_data_loader, val_data_loader\n",
        "\n",
        "\n",
        "    else:\n",
        "        test_dataset_transformed = preprocess.TransformWrapper(_dataset, val_transform)\n",
        "        data_loader = torch.utils.data.DataLoader(\n",
        "            test_dataset_transformed, batch_size=batch_size, shuffle=is_train, num_workers=1\n",
        "        )\n",
        "        return data_loader\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validation(m, ds):\n",
        "    num_data = 0\n",
        "    corrects = 0\n",
        "\n",
        "    # Test loop\n",
        "    m.net.eval()\n",
        "    _softmax = torch.nn.Softmax(dim=1)\n",
        "    for i, data in enumerate(tqdm(ds)):\n",
        "        if i == 0:\n",
        "            print(f\"Data structure: {type(data)}\")\n",
        "            print(f\"Data length: {len(data)}\")\n",
        "            images, labels, _ = data\n",
        "            print(f\"Images shape: {images.shape}\")\n",
        "            print(f\"Labels shape: {labels.shape}\")\n",
        "            print(f\"Unique labels in batch: {torch.unique(labels)}\")\n",
        "        images, labels, _ = data\n",
        "\n",
        "        images = images.to(m.device)\n",
        "        labels = labels.to(m.device)\n",
        "\n",
        "        predictions = m.inference(images)\n",
        "        predictions = predictions.to(m.device)\n",
        "        predictions = _softmax(predictions)\n",
        "\n",
        "        _, predictions = torch.max(predictions.data, 1)\n",
        "\n",
        "        # DEBUG: Check predictions\n",
        "        if i == 0:\n",
        "            print(f\"Predicted classes: {predictions[:10]}\")\n",
        "            print(f\"True labels: {labels[:10]}\")\n",
        "            print(f\"Matches: {(predictions == labels)[:10]}\")\n",
        "\n",
        "        labels = labels.type(torch.LongTensor)\n",
        "        num_data += labels.size(0)\n",
        "        corrects += (predictions == labels.to(m.device)).sum().item()\n",
        "\n",
        "    accuracy = 100 * corrects / num_data\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fb55a890",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb55a890",
        "outputId": "260b91ef-dce0-47c8-e81b-a8b8465c598a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "load train data set: 100%|██████████| 2747/2747 [00:01<00:00, 2141.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Augmenting training data with patches...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train augmentation: 100%|██████████| 2747/2747 [00:12<00:00, 224.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Résultats augmentation :\n",
            "  Train : 2747 images → 134603 patches\n",
            "  Facteur : ~49x (13x13 = 169 patches/image)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "First batch shapes:\n",
            "  Images: torch.Size([100, 1, 94, 94]), dtype: torch.float32\n",
            "  Labels: torch.Size([100]), dtype: torch.int64\n",
            "  Labels values: [3, 9, 5, 8, 2, 4, 5, 9, 3, 6]\n",
            "  Unique labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "load test data set: 100%|██████████| 2425/2425 [00:01<00:00, 1430.20it/s]\n"
          ]
        }
      ],
      "source": [
        "# Décomposition de la fonction run\n",
        "\n",
        "# Load data\n",
        "train_set, val_set = load_dataset(DATA_PATH, True, dataset, batch_size)\n",
        "test_set = load_dataset(DATA_PATH, False, dataset, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "DVTHgW90ODk6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "DVTHgW90ODk6",
        "outputId": "23d8f0fb-8c14-46f8-c0b1-dac3ce6c535a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device used: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|██▊       | 306/1077 [05:02<12:41,  1.01it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2784050038.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0m_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ATR-Code/src/models/AConvNet/_base.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "m = AConvNet.Model(\n",
        "    classes=classes, dropout_rate=dropout_rate, channels=channels,\n",
        "    lr=lr, lr_step=lr_step, lr_decay=lr_decay,\n",
        "    weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "model_path = os.path.join(experiments_path, f'{model_str}/models/{model_name}')\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "history_path = os.path.join(experiments_path, f'{model_str}/history')\n",
        "if not os.path.exists(history_path):\n",
        "    os.makedirs(history_path, exist_ok=True)\n",
        "\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_accuracy': [],\n",
        "    'val_loss': [],\n",
        "    'val_accuracy': []\n",
        "}\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    _loss = []\n",
        "\n",
        "    m.net.train()\n",
        "    for i, data in enumerate(tqdm(train_set)):\n",
        "        images, labels, _ = data\n",
        "        _loss.append(m.optimize(images, labels))\n",
        "\n",
        "    if m.lr_scheduler:\n",
        "        lr = m.lr_scheduler.get_last_lr()[0]\n",
        "        m.lr_scheduler.step()\n",
        "\n",
        "    train_accuracy = validation(m, train_set)\n",
        "    val_accuracy = validation(m, val_set)\n",
        "\n",
        "    print(\n",
        "        f'Epoch: {epoch + 1:03d}/{epochs:03d} | loss={np.mean(_loss):.4f} | lr={lr} | Train accuracy={train_accuracy:.2f} | Validation accuracy={val_accuracy:.2f}'\n",
        "    )\n",
        "\n",
        "    history['train_loss'].append(np.mean(_loss))\n",
        "    history['train_accuracy'].append(train_accuracy)\n",
        "    history['val_accuracy'].append(val_accuracy)\n",
        "\n",
        "    if experiments_path:\n",
        "        m.save(os.path.join(model_path, f'model-{epoch + 1:03d}.pth'))\n",
        "\n",
        "    with open(os.path.join(history_path, f'history-{model_name}.json'), mode='w', encoding='utf-8') as f:\n",
        "        json.dump(history, f, ensure_ascii=True, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JPJNexoOiqCe",
      "metadata": {
        "id": "JPJNexoOiqCe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PIE-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
