{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e69b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "from absl import flags\n",
    "from absl import app\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils import tensorboard\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get the project root\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Add src/ to path\n",
    "sys.path.append(os.path.join(project_root, \"src\"))\n",
    "\n",
    "# modules in src\n",
    "from data.MSTAR.paper_AConvNet import preprocess\n",
    "from data.MSTAR.paper_AConvNet import loader\n",
    "from utils import common\n",
    "from models import AConvNet\n",
    "\n",
    "DATA_PATH = 'datasets/MSTAR/MSTAR_IMG_JSON'\n",
    "\n",
    "# DATA_PATH = 'datasets/MSTAR/mstar_data_paper_AConvNet/'\n",
    "\n",
    "model_str = 'AConvNet'\n",
    "\n",
    "\n",
    "# flags.DEFINE_string('experiments_path', os.path.join(common.project_root, 'experiments'), help='')\n",
    "# flags.DEFINE_string('config_name', f'{model_str}/config/AConvNet-SOC.json', help='')\n",
    "# FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "common.set_random_seed(12321)\n",
    "\n",
    "def load_dataset(path, is_train, name, batch_size):\n",
    "\n",
    "    val_transform = torchvision.transforms.Compose([preprocess.CenterCrop(88), torchvision.transforms.ToTensor()])\n",
    "\n",
    "    train_transform = torchvision.transforms.Compose([preprocess.RandomCrop(88), torchvision.transforms.ToTensor()])\n",
    "\n",
    "    _dataset = loader.Dataset(\n",
    "        path, name=name, is_train=is_train,\n",
    "        transform=None\n",
    "    )\n",
    "\n",
    "    if is_train:\n",
    "        # TODO add data_augmentation (in preprocess file)\n",
    "        # Split into train (80%) and validation (20%)\n",
    "        train_size = int(0.8 * len(_dataset))\n",
    "        val_size = len(_dataset) - train_size\n",
    "\n",
    "        train_dataset, val_dataset = random_split(_dataset, [train_size, val_size])\n",
    "\n",
    "        # CenterCrop for val and RandomCrop for train\n",
    "        train_dataset_transformed = preprocess.TransformWrapper(train_dataset, train_transform)\n",
    "        val_dataset_transformed = preprocess.TransformWrapper(val_dataset, val_transform)\n",
    "\n",
    "        train_data_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset_transformed, batch_size=batch_size, shuffle=is_train, num_workers=1\n",
    "        )\n",
    "\n",
    "        val_data_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset_transformed, batch_size=batch_size, shuffle=False, num_workers=1\n",
    "        )\n",
    "\n",
    "        return train_data_loader, val_data_loader\n",
    "\n",
    "\n",
    "    else:\n",
    "        test_dataset_transformed = preprocess.TransformWrapper(_dataset, val_transform)\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset_transformed, batch_size=batch_size, shuffle=is_train, num_workers=1\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation(m, ds):\n",
    "    num_data = 0\n",
    "    corrects = 0\n",
    "\n",
    "    # Test loop\n",
    "    m.net.eval()\n",
    "    _softmax = torch.nn.Softmax(dim=1)\n",
    "    for i, data in enumerate(tqdm(ds)):\n",
    "        images, labels, _ = data\n",
    "\n",
    "        predictions = m.inference(images)\n",
    "        predictions = _softmax(predictions)\n",
    "\n",
    "        _, predictions = torch.max(predictions.data, 1)\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        num_data += labels.size(0)\n",
    "        corrects += (predictions == labels.to(m.device)).sum().item()\n",
    "\n",
    "    accuracy = 100 * corrects / num_data\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def run(epochs, dataset, classes, channels, batch_size,\n",
    "        lr, lr_step, lr_decay, weight_decay, dropout_rate,\n",
    "        model_name, experiments_path=None):\n",
    "    train_set = load_dataset(DATA_PATH, True, dataset, batch_size)\n",
    "    valid_set = load_dataset(DATA_PATH, False, dataset, batch_size)\n",
    "\n",
    "    m = AConvNet.Model(\n",
    "        classes=classes, dropout_rate=dropout_rate, channels=channels,\n",
    "        lr=lr, lr_step=lr_step, lr_decay=lr_decay,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model_path = os.path.join(experiments_path, f'{model_str}/models/{model_name}')\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "    history_path = os.path.join(experiments_path, f'{model_str}/history')\n",
    "    if not os.path.exists(history_path):\n",
    "        os.makedirs(history_path, exist_ok=True)\n",
    "\n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'accuracy': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        _loss = []\n",
    "\n",
    "        m.net.train()\n",
    "        for i, data in enumerate(tqdm(train_set)):\n",
    "            images, labels, _ = data\n",
    "            _loss.append(m.optimize(images, labels))\n",
    "\n",
    "        if m.lr_scheduler:\n",
    "            lr = m.lr_scheduler.get_last_lr()[0]\n",
    "            m.lr_scheduler.step()\n",
    "\n",
    "        accuracy = validation(m, valid_set)\n",
    "\n",
    "        logging.info(\n",
    "            f'Epoch: {epoch + 1:03d}/{epochs:03d} | loss={np.mean(_loss):.4f} | lr={lr} | accuracy={accuracy:.2f}'\n",
    "        )\n",
    "\n",
    "        history['loss'].append(np.mean(_loss))\n",
    "        history['accuracy'].append(accuracy)\n",
    "\n",
    "        if experiments_path:\n",
    "            m.save(os.path.join(model_path, f'model-{epoch + 1:03d}.pth'))\n",
    "\n",
    "    with open(os.path.join(history_path, f'history-{model_name}.json'), mode='w', encoding='utf-8') as f:\n",
    "        json.dump(history, f, ensure_ascii=True, indent=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e73ef2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Start')\n",
    "# experiments_path = FLAGS.experiments_path\n",
    "# config_name = FLAGS.config_name\n",
    "\n",
    "# config = common.load_config(os.path.join(experiments_path, config_name))\n",
    "\n",
    "experiments_path = os.path.join(common.project_root, 'experiments')\n",
    "\n",
    "config = {\n",
    "  \"model_name\": \"AConvNet-SOC\",\n",
    "  \"dataset\": \"SOC\",\n",
    "  \"num_classes\": 10,\n",
    "  \"channels\": 1,\n",
    "  \"batch_size\": 100,\n",
    "  \"epochs\": 2,\n",
    "  \"momentum\": 0.9,\n",
    "  \"lr\": 1e-3,\n",
    "  \"lr_step\": [50],\n",
    "  \"lr_decay\": 0.1,\n",
    "  \"weight_decay\": 4e-3,\n",
    "  \"dropout_rate\": 0.5\n",
    "}\n",
    "\n",
    "dataset = config['dataset']\n",
    "classes = config['num_classes']\n",
    "channels = config['channels']\n",
    "epochs = config['epochs']\n",
    "batch_size = config['batch_size']\n",
    "\n",
    "lr = config['lr']\n",
    "lr_step = config['lr_step']\n",
    "lr_decay = config['lr_decay']\n",
    "\n",
    "weight_decay = config['weight_decay']\n",
    "dropout_rate = config['dropout_rate']\n",
    "\n",
    "model_name = config['model_name']\n",
    "\n",
    "# run(epochs, dataset, classes, channels, batch_size,\n",
    "#     lr, lr_step, lr_decay, weight_decay, dropout_rate,\n",
    "#     model_name, experiments_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb55a890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load train data set:   0%|          | 0/2747 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load train data set: 100%|██████████| 2747/2747 [00:00<00:00, 4567.85it/s]\n",
      "load test data set: 100%|██████████| 2425/2425 [00:00<00:00, 4127.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# Décomposition de la fonction run\n",
    "\n",
    "# Load data\n",
    "train_set, val_set = load_dataset(DATA_PATH, True, dataset, batch_size)\n",
    "test_set = load_dataset(DATA_PATH, False, dataset, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PIE-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
